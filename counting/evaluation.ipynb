{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "196a2bc3-95d4-4a58-97e9-7e273b1c1347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#from yolov5 import train, val, detect\n",
    "#from yolov5.utils import metrics\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import torch\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8f2641a-370f-4122-a945-5f8aaed02a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from cv2 import cv2\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52af2326-3bfa-412c-b45c-e4485242400b",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_LOCATION = \"/media/danil-pass123/E/DataSet/train1/train\"\n",
    "DATA = \"/media/danil-pass123/E/DataSet\"\n",
    "\n",
    "weights = DATA + '/exp/exp/weights/best.pt'\n",
    "runs_location = DATA + \"/runs_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcadfa01-c063-4921-9a21-fb173fc00e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to disable warnings of pandas assignment\n",
    "pd.options.mode.chained_assignment = None\n",
    "def predict_bboxes(img):\n",
    "    \"\"\"\n",
    "    predicting bboxes for all images using yolo model\n",
    "    :param img: list[Image.Image]\n",
    "    :return: list[pd.DatafFrame]\n",
    "    \"\"\"\n",
    "    def make_bbox(df):\n",
    "        \"\"\"\n",
    "        filter bbox by confidence\n",
    "        :param df: pd.DataFrame\n",
    "        :return: dataframe with bbox\n",
    "        \"\"\"\n",
    "        filtered_df = df[df['confidence'] > 0.3]\n",
    "        bbox = filtered_df[['xmin', 'ymin', 'xmax', 'ymax']]\n",
    "        return bbox\n",
    "    \n",
    "    data = model(img)\n",
    "    bboxes = [make_bbox(im) for im in data.pandas().xyxy]\n",
    "    \n",
    "    return bboxes\n",
    "\n",
    "\n",
    "def count_animals(old_bboxes, new_bboxes, image1, image2):\n",
    "    \"\"\"\n",
    "    Count animals by difference between 2 dataframe with bboxes.\n",
    "    It returns number of new animals in new image\n",
    "    :param old_bboxes: pd.DataFrame\n",
    "    :param new_bboxes: pd.DataFrame\n",
    "    :return: int\n",
    "    \"\"\"\n",
    "    \n",
    "    def image_diff(bbox1,img1,img2):\n",
    "        x_min1,y_min1,x_max1,y_max1 = bbox1.values[-1].astype(int)\n",
    "        \n",
    "        x_min = max(x_min1-10,0)\n",
    "        y_min = max(y_min1-10,0)\n",
    "        x_max = x_max1+10\n",
    "        y_max = y_max1+10\n",
    "        \n",
    "        img1,img2 = convert_tensor(img1),convert_tensor(img2)\n",
    "        \n",
    "        difference = img1[:,x_min:x_max,y_min:y_max] - img2[:,x_min:x_max,y_min:y_max]\n",
    "        n_cells = ((x_max - x_min)*(y_max - y_min))\n",
    "        \n",
    "        return (difference).sum()/n_cells\n",
    "    \n",
    "    def new_obj_cond(bbox,img1,img2):\n",
    "        if image_diff(bbox,img1,img2)<0.3:\n",
    "            return False\n",
    "        return True\n",
    "            \n",
    "    \n",
    "    def exit_obj_cond(bbox,img1,img2):\n",
    "        if image_diff(bbox,img1,img2)<0.3:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def cond(x, y) -> bool:\n",
    "        \"\"\"\n",
    "        return condition whether 2 bboxes are similar\n",
    "        Similarity can be changed, but for now it is euclidean distance between bboxes less than threshold.\n",
    "        :param x: iloc indexer of pd.DataFrame, indicating bbox\n",
    "        :param y: iloc indexer of pd.DataFrame, indicating bbox\n",
    "        :return: bool\n",
    "        \"\"\"\n",
    "        threshold = 20\n",
    "        x = torch.from_numpy(x.to_numpy())\n",
    "        y = torch.from_numpy(y.to_numpy())\n",
    "        x = x[[0,1]]\n",
    "        y = y[[0,1]]\n",
    "        s = (torch.sum((x - y)**2)**0.5).item()\n",
    "        return s < threshold\n",
    "    \"\"\"\n",
    "    def tracking(traker, image1, image2, box: pd.DataFrame) -> bool:\n",
    "        box = (box['xmin'], box['ymin'], box['xmax'] - box['xmin'], box['ymax'] - box['ymin'])\n",
    "        frame1 = cv2.cvtColor(np.array(image1), cv2.COLOR_RGB2GRAY)\n",
    "        tracker.init(frame1, box)\n",
    "        frame2 = cv2.cvtColor(np.array(image2), cv2.COLOR_RGB2GRAY)\n",
    "        status, bbox = traker.update(frame2)\n",
    "        return status\n",
    "    \"\"\"\n",
    "    #temporary data structures that saves whether certain bbox in similar to bbox from previous image\n",
    "    bbox = new_bboxes.copy()\n",
    "    prev_bbox = old_bboxes.copy()\n",
    "    prev_bbox['prev'] = False\n",
    "    bbox['prev'] = False\n",
    "    \n",
    "    \n",
    "    #mapping\n",
    "    for i in range(len(old_bboxes)):\n",
    "        for j in range(len(new_bboxes)):\n",
    "            #if bbox is already similar to bbox from previous image, then we don't need to check this bbox\n",
    "            if bbox.iloc[j]['prev'] or prev_bbox.iloc[i]['prev']:\n",
    "                continue\n",
    "            #similarity between bboxes from previous and present images\n",
    "            if cond(old_bboxes.iloc[i], new_bboxes.iloc[j]):\n",
    "                #save status of similarity\n",
    "                prev_bbox['prev'][i] = True\n",
    "                bbox['prev'][j] = True\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            tracker = cv2.TrackerKCF_create()\n",
    "            if tracking(tracker, image1, image2, old_bboxes.iloc[i].astype(int)):\n",
    "                print('tracking')\n",
    "                prev_bbox['prev'][i] = True\n",
    "                bbox['prev'][j] = True\n",
    "            \"\"\"\n",
    "    next_bbox = prev_bbox[prev_bbox['prev']==True].reindex().copy()\n",
    "    print(next_bbox)\n",
    "    \n",
    "    bbox = bbox[bbox['prev']==False].reindex().copy()\n",
    "    print(bbox)\n",
    "    for i in range(len(bbox)):\n",
    "        if new_obj_cond(bbox.iloc[i],image1, image2):\n",
    "            next_bbox = np.concatentate([next_bbox,bbox.iloc[i]])\n",
    "            \n",
    "    print(next_bbox)\n",
    "    \n",
    "    prev_bbox = prev_bbox[prev_bbox['prev']==False].reindex().copy()\n",
    "    for i in range(len(prev_bbox)):\n",
    "        if exit_obj_cond(prev_bbox.iloc[i],image1, image2):\n",
    "        \n",
    "    #return number of non-similar(new animals) bboxes\n",
    "    return bbox[bbox['prev'] == False].size.item()\n",
    "\n",
    "\n",
    "#def predict_sequence(images: list[Image.Image]):\n",
    "def predict_sequence(images):\n",
    "    bboxes = predict_bboxes(images)\n",
    "    #first image's bboxes are all animals\n",
    "    count = bboxes[0].shape[0]\n",
    "    for i in range(len(bboxes) - 1):\n",
    "        #check the difference between two images and return new animals\n",
    "        count += count_animals(bboxes[i], bboxes[i + 1], images[i], images[i + 1])\n",
    "    return count\n",
    "\n",
    "#def predict(files: list[str]):\n",
    "def predict(files):\n",
    "    \"\"\"\n",
    "    Prediction of number of animals in sequence of images.\n",
    "    :param files: list[str], str is path to file\n",
    "    :return: int\n",
    "    \"\"\"\n",
    "    try:\n",
    "        images = [Image.open(file) for file in files]\n",
    "    except:\n",
    "        return -1\n",
    "    return predict_sequence(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f057953-af0a-4a8b-a064-eb7056caa654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_seq(seq_id: str) -> int:\n",
    "    \"\"\"\n",
    "    Run predictions for certain sequence providing seq_id.\n",
    "    :param seq_id: sequence\n",
    "    :return: int\n",
    "    \"\"\"\n",
    "    images = pd.read_csv(DATA+'/processed_train.csv', sep='\\t', index_col=0)\n",
    "    \n",
    "    seq = images[images['seq_id'] == seq_id]\n",
    "    image_dir = IMAGES_LOCATION + '/'\n",
    "    files = [image_dir + file for file in seq.sort_values(by='seq_frame_num')['file_name'].tolist()]\n",
    "    counts = predict(files)\n",
    "    #collect_sequence_metadate(seq_id,counts)\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea16a5b5-29cf-441e-9fd6-e1342a94a507",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c9dee4a-42b8-45db-8ce9-c1c8477f7358",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_data = pd.read_csv(DATA + \"/seqs.csv\",sep='\\t',index_col=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e4c9914-575f-4335-b522-eed64843b82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_data = pd.read_csv(DATA+\"/train_sequence_counts.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91cbfe70-bc12-45c7-87d6-916574818ddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.035104155185716"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = 0\n",
    "mse_array = []\n",
    "num_of_object_count = len(seq_data.index)\n",
    "for idx in true_data.index:\n",
    "    if pd.isnull(seq_data.loc[idx]['animal_count']) or pd.isnull(true_data.loc[idx]['count']):\n",
    "        print(idx)\n",
    "    else:\n",
    "        #print((seq_data.loc[idx]['animal_count'] - true_data.loc[idx]['count']))\n",
    "        val = (seq_data.loc[idx]['animal_count'] - true_data.loc[idx]['count'])\n",
    "        mse_array.append(val)\n",
    "        mse += val\n",
    "        #num_of_object_count+=1\n",
    "mse = mse/num_of_object_count\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83fb2f08-111f-4441-a22a-63c04297a68a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(range(len(mse_array)),mse_array)\n",
    "plt.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
